

=== ./src/models/analysis.rs ===

use std::time::Duration;
use std::sync::atomic::{AtomicUsize, Ordering};
use super::FileInfo;

#[derive(Debug)]
pub struct ZipAnalysis {
    pub files: Vec<FileInfo>,
    pub total_size: u64,
    pub compressed_size: u64,
    pub compression_ratio: f64,
    pub stats: AnalysisStats,
}

#[derive(Debug, Clone)]
pub struct AnalysisStats {
    pub duration: Duration,
    pub chunks_processed: AtomicUsize,
    pub error_count: AtomicUsize,
    pub peak_memory_mb: AtomicUsize,
}

impl AnalysisStats {
    pub fn new() -> Self {
        Self {
            duration: Duration::default(),
            chunks_processed: AtomicUsize::new(0),
            error_count: AtomicUsize::new(0),
            peak_memory_mb: AtomicUsize::new(0),
        }
    }
}

impl ZipAnalysis {
    pub fn new(files: Vec<FileInfo>, stats: AnalysisStats) -> Self {
        let total_size: u64 = files.iter().map(|f| f.size).sum();
        let compressed_size: u64 = files.iter().map(|f| f.compressed_size).sum();
        let compression_ratio = if total_size > 0 {
            compressed_size as f64 / total_size as f64
        } else {
            1.0
        };

        Self {
            files,
            total_size,
            compressed_size,
            compression_ratio,
            stats,
        }
    }

    pub fn file_count(&self) -> usize {
        self.files.len()
    }

    pub fn total_savings(&self) -> u64 {
        self.total_size.saturating_sub(self.compressed_size)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::CompressionMethod;
    use std::path::PathBuf;
    use chrono::Utc;

    #[test]
    fn test_analysis_calculations() {
        let files = vec![
            FileInfo {
                path: PathBuf::from("test1.txt"),
                size: 1000,
                compressed_size: 500,
                compression_method: CompressionMethod::Deflated,
                crc32: 0,
                modified: Utc::now(),
            },
            FileInfo {
                path: PathBuf::from("test2.txt"),
                size: 2000,
                compressed_size: 1000,
                compression_method: CompressionMethod::Deflated,
                crc32: 0,
                modified: Utc::now(),
            },
        ];

        let analysis = ZipAnalysis::new(files, AnalysisStats::new());

        assert_eq!(analysis.total_size, 3000);
        assert_eq!(analysis.compressed_size, 1500);
        assert_eq!(analysis.compression_ratio, 0.5);
        assert_eq!(analysis.file_count(), 2);
        assert_eq!(analysis.total_savings(), 1500);
    }
}


=== ./src/models/file_info.rs ===

use std::path::PathBuf;
use chrono::{DateTime, Utc};
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use crate::error::AnalysisError;

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct FileInfo {
    pub path: PathBuf,
    pub size: u64,
    pub compressed_size: u64,
    pub compression_method: CompressionMethod,
    pub crc32: u32,
    pub modified: DateTime<Utc>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CompressionMethod {
    Stored,
    Deflated,
    Other(u16),
}

#[derive(Debug)]
pub struct ProcessingStats {
    pub bytes_processed: AtomicU64,
    pub files_processed: AtomicUsize,
    pub errors_encountered: AtomicUsize,
}

impl ProcessingStats {
    pub fn new() -> Self {
        Self {
            bytes_processed: AtomicU64::new(0),
            files_processed: AtomicUsize::new(0),
            errors_encountered: AtomicUsize::new(0),
        }
    }

    pub fn increment_bytes(&self, bytes: u64) {
        self.bytes_processed.fetch_add(bytes, Ordering::Relaxed);
    }

    pub fn increment_files(&self) {
        self.files_processed.fetch_add(1, Ordering::Relaxed);
    }

    pub fn increment_errors(&self) {
        self.errors_encountered.fetch_add(1, Ordering::Relaxed);
    }
}

impl From<zip::CompressionMethod> for CompressionMethod {
    fn from(method: zip::CompressionMethod) -> Self {
        match method {
            zip::CompressionMethod::Stored => Self::Stored,
            zip::CompressionMethod::Deflated => Self::Deflated,
            other => Self::Other(other.to_u16()),
        }
    }
}

impl std::fmt::Display for CompressionMethod {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Stored => write!(f, "Stored"),
            Self::Deflated => write!(f, "Deflated"),
            Self::Other(n) => write!(f, "Method({})", n)
        }
    }
}

impl TryFrom<u16> for CompressionMethod {
    type Error = AnalysisError;

    fn try_from(value: u16) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Stored),
            8 => Ok(Self::Deflated),
            other => Ok(Self::Other(other)),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_file_info_creation() {
        let info = FileInfo {
            path: Path::new("test.txt").to_path_buf(),
            size: 100,
            compressed_size: 50,
            compression_method: CompressionMethod::Deflated,
            crc32: 12345,
            modified: Utc::now(),
        };
        assert_eq!(info.compression_method, CompressionMethod::Deflated);
        assert_eq!(info.size, 100);
        assert_eq!(info.compressed_size, 50);
    }

    #[test]
    fn test_processing_stats_thread_safety() {
        let stats = ProcessingStats::new();
        stats.increment_bytes(100);
        stats.increment_files();
        assert_eq!(stats.bytes_processed.load(Ordering::Relaxed), 100);
        assert_eq!(stats.files_processed.load(Ordering::Relaxed), 1);
    }
}


=== ./src/models/mod.rs ===

mod file_info;
mod analysis;

pub use file_info::{FileInfo, CompressionMethod, ProcessingStats};
pub use analysis::{ZipAnalysis, AnalysisStats};


=== ./src/error.rs ===

use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum AnalysisError {
    #[error("IO error at offset {offset}: {source}")]
    Io { 
        source: Box<std::io::Error>,
        offset: u64 
    },
    
    #[error("ZIP error: {source}")]
    Zip { 
        source: Box<zip::result::ZipError>
    },
    
    #[error("Invalid input: {reason}")]
    InvalidInput { 
        reason: String 
    },
    
    #[error("Memory limit exceeded: needed {required}MB, limit {max_allowed}MB")]
    Memory { 
        required: u64, 
        max_allowed: u64,
        current_usage: u64,
    },

    #[error("Corrupt ZIP at {offset}, processed {processed_bytes} bytes")]
    Corrupt { 
        offset: u64, 
        processed_bytes: u64,
        partial_results: Option<PartialAnalysis>,
        recovery_possible: bool,
    },

    #[error("Channel error: {msg}")]
    Channel { 
        msg: String 
    },

    #[error("Task cancelled")]
    Cancelled,

    #[error("Other error: {source}")]
    Other { 
        source: String 
    },
}

#[derive(Debug, Clone)]
pub struct PartialAnalysis {
    pub processed_files: Vec<PathBuf>,
    pub bytes_processed: u64,
    pub is_recoverable: bool,
}

impl AnalysisError {
    pub fn is_recoverable(&self) -> bool {
        matches!(self,
            Self::Memory { .. } |
            Self::Corrupt { recovery_possible: true, .. } |
            Self::Channel { .. }
        )
    }

    pub fn should_retry(&self) -> bool {
        matches!(self, 
            Self::Memory { .. } |
            Self::Channel { .. }
        )
    }
}

pub type Result<T> = std::result::Result<T, AnalysisError>;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_error_recovery_logic() {
        let recoverable = AnalysisError::Memory {
            required: 100,
            max_allowed: 50,
            current_usage: 75,
        };
        assert!(recoverable.is_recoverable());
        assert!(recoverable.should_retry());

        let unrecoverable = AnalysisError::Corrupt {
            offset: 0,
            processed_bytes: 100,
            partial_results: None,
            recovery_possible: false,
        };
        assert!(!unrecoverable.is_recoverable());
        assert!(!unrecoverable.should_retry());
    }
}


=== ./src/main.rs ===

use std::path::PathBuf;
use anyhow::Context;
use clap::Parser;
use tokio::{signal::ctrl_c, fs::metadata};
use crate::{
    analyzer::ParallelZipAnalyzer,
    writer::{ReportWriter, FormatConfig, ProgressTracker, ProgressConfig},
    error::Result,
};

mod analyzer;
mod error;
mod models;
mod writer;

#[derive(Parser)]
#[command(author, version, about = "Parallel ZIP file analyzer")]
struct Args {
    /// Input ZIP file path
    input: PathBuf,

    /// Output report file path
    output: PathBuf,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();

    // Validate input file
    let file_size = metadata(&args.input)
        .await
        .context("Failed to read input file")?
        .len();

    // Create analyzer
    let analyzer = ParallelZipAnalyzer::new(args.input.clone());
    
    // Setup progress tracking
    let progress = ProgressTracker::new(
        file_size,
        ProgressConfig::default(),
    );

    // Setup report writer
    let writer = ReportWriter::new(args.output, FormatConfig::default());

    // Handle Ctrl+C
    let progress_clone = progress.clone();
    tokio::spawn(async move {
        if let Ok(()) = ctrl_c().await {
            progress_clone.handle_interrupt().unwrap_or_else(|e| {
                eprintln!("Error during interrupt: {}", e);
            });
            std::process::exit(130); // Standard SIGINT exit code
        }
    });

    // Run analysis
    let analysis = analyzer.analyze().await?;
    
    // Write report
    writer.write(&analysis).await?;
    
    // Finish progress
    progress.finish()?;

    println!("Analysis complete! Processed {} files ({} bytes)", 
        analysis.files.len(),
        analysis.total_size
    );
    Ok(())
}


=== ./src/analyzer/chunks.rs ===

use std::{
    path::PathBuf,
    sync::{Arc, atomic::{AtomicUsize, AtomicU64, Ordering}},
    io::Cursor,
};
use tokio::sync::Semaphore;
use rayon::prelude::*;
use crate::{
    error::{Result, AnalysisError},
    models::{FileInfo, CompressionMethod, ZipAnalysis, AnalysisStats},
};

pub struct ChunkConfig {
    pub chunk_size: usize,
    pub buffer_count: usize,
    pub buffer_size: usize,
    pub memory_limit: usize,
}

impl Default for ChunkConfig {
    fn default() -> Self {
        Self {
            chunk_size: 16 * 1024 * 1024,  // 16MB chunks
            buffer_count: num_cpus::get() * 2,
            buffer_size: 8 * 1024 * 1024,  // 8MB buffers
            memory_limit: 1024 * 1024 * 1024,  // 1GB
        }
    }
}

#[derive(Debug)]
pub struct ChunkStats {
    pub processed_chunks: AtomicUsize,
    pub total_bytes: AtomicU64,
    pub active_threads: AtomicUsize,
}

impl ChunkStats {
    pub fn new() -> Self {
        Self {
            processed_chunks: AtomicUsize::new(0),
            total_bytes: AtomicU64::new(0),
            active_threads: AtomicUsize::new(0),
        }
    }
}

pub struct ChunkResult {
    pub offset: u64,
    pub files: Vec<FileInfo>,
    pub compressed_size: u64,
    pub uncompressed_size: u64,
    pub error: Option<AnalysisError>,
}

pub struct BufferPool {
    buffers: Arc<Semaphore>,
    buffer_size: usize,
}

impl BufferPool {
    pub fn new(config: &ChunkConfig) -> Self {
        Self {
            buffers: Arc::new(Semaphore::new(config.buffer_count)),
            buffer_size: config.buffer_size,
        }
    }

    pub async fn acquire(&self) -> Result<Vec<u8>> {
        self.buffers.acquire().await.map_err(|e| {
            AnalysisError::Channel { 
                msg: format!("Failed to acquire buffer: {}", e) 
            }
        })?;
        
        Ok(vec![0; self.buffer_size])
    }

    pub fn release(&self, _buffer: Vec<u8>) {
        self.buffers.add_permits(1);
    }
}

pub struct ChunkProcessor {
    config: ChunkConfig,
    buffer_pool: BufferPool,
    stats: Arc<ChunkStats>,
}

impl ChunkProcessor {
    pub fn new(config: ChunkConfig) -> Self {
        Self {
            buffer_pool: BufferPool::new(&config),
            stats: Arc::new(ChunkStats::new()),
            config,
        }
    }

    pub async fn process_chunk(&self, chunk: &[u8], offset: u64) -> Result<ChunkResult> {
        self.stats.active_threads.fetch_add(1, Ordering::SeqCst);
        let _buffer = self.buffer_pool.acquire().await?;
        
        let result = rayon::scope(|s| -> Result<ChunkResult> {
            s.spawn(|_| {
                let cursor = Cursor::new(chunk);
                let mut zip = zip::ZipArchive::new(cursor)
                    .map_err(|e| AnalysisError::Zip { 
                        source: Box::new(e) 
                    })?;
                
                let mut files = Vec::new();
                let mut compressed_size = 0;
                let mut uncompressed_size = 0;

                for i in 0..zip.len() {
                    let file = zip.by_index(i)
                        .map_err(|e| AnalysisError::Zip { 
                            source: Box::new(e) 
                        })?;
                    
                    files.push(FileInfo {
                        path: PathBuf::from(file.name()),
                        size: file.size(),
                        compressed_size: file.compressed_size(),
                        compression_method: file.compression().into(),
                        crc32: file.crc32(),
                        modified: chrono::DateTime::from(
                            file.last_modified().to_time().unwrap()
                        ),
                    });

                    compressed_size += file.compressed_size();
                    uncompressed_size += file.size();
                }

                Ok(ChunkResult {
                    offset,
                    files,
                    compressed_size,
                    uncompressed_size,
                    error: None,
                })
            }).join().unwrap()
        });

        self.buffer_pool.release(_buffer);
        self.stats.active_threads.fetch_sub(1, Ordering::SeqCst);
        self.stats.processed_chunks.fetch_add(1, Ordering::SeqCst);
        self.stats.total_bytes.fetch_add(chunk.len() as u64, Ordering::SeqCst);

        result
    }

    pub fn merge_results(results: &[ChunkResult]) -> Result<ZipAnalysis> {
        let mut all_files = Vec::new();
        let mut total_compressed = 0;
        let mut total_uncompressed = 0;
        let mut error_count = 0;

        for result in results {
            if let Some(error) = &result.error {
                error_count += 1;
                if !error.is_recoverable() {
                    return Err(error.clone());
                }
            }
            all_files.extend(result.files.clone());
            total_compressed += result.compressed_size;
            total_uncompressed += result.uncompressed_size;
        }

        let stats = AnalysisStats {
            duration: Default::default(),
            chunks_processed: AtomicUsize::new(results.len()),
            error_count: AtomicUsize::new(error_count),
            peak_memory_mb: AtomicUsize::new(0),
        };

        Ok(ZipAnalysis::new(all_files, stats))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::io::Write;

    fn create_test_zip() -> Vec<u8> {
        let mut buf = Vec::new();
        let mut zip = zip::ZipWriter::new(Cursor::new(&mut buf));
        
        let options = zip::write::FileOptions::default()
            .compression_method(zip::CompressionMethod::Stored);
        
        zip.start_file("test.txt", options).unwrap();
        zip.write_all(b"Hello, World!").unwrap();
        zip.finish().unwrap();
        
        buf
    }

    #[tokio::test]
    async fn test_chunk_processing() {
        let processor = ChunkProcessor::new(ChunkConfig::default());
        let test_data = create_test_zip();
        
        let result = processor.process_chunk(&test_data, 0).await.unwrap();
        
        assert_eq!(result.files.len(), 1);
        assert_eq!(result.files[0].path.to_str().unwrap(), "test.txt");
        assert_eq!(result.files[0].size, 13);
        assert_eq!(result.compressed_size, 13);
    }
}


=== ./src/analyzer/zip.rs ===

use std::path::PathBuf;
use tokio::{
    sync::{mpsc, watch},
    io::{AsyncReadExt, BufReader},
    time::Duration,
};
use crate::{
    error::{Result, AnalysisError},
    models::ZipAnalysis,
    writer::progress::ProgressUpdate,
};
use super::{
    chunks::{ChunkConfig, ChunkProcessor, ChunkResult},
};

#[derive(Debug, Clone)]
pub enum ControlSignal {
    Continue,
    Pause,
    Stop { graceful: bool },
}

pub struct AnalyzerChannels {
    progress_tx: mpsc::Sender<ProgressUpdate>,
    chunk_results_tx: mpsc::Sender<ChunkResult>,
    control_tx: watch::Sender<ControlSignal>,
}

pub struct ParallelZipAnalyzer {
    zip_path: PathBuf,
    chunk_size: usize,
    thread_count: usize,
    chunk_processor: ChunkProcessor,
}

impl ParallelZipAnalyzer {
    pub fn new(zip_path: PathBuf) -> Self {
        let config = ChunkConfig::default();
        Self {
            zip_path,
            chunk_size: config.chunk_size,
            thread_count: num_cpus::get() - 1,
            chunk_processor: ChunkProcessor::new(config),
        }
    }

    pub async fn analyze(&self) -> Result<ZipAnalysis> {
        let start_time = std::time::Instant::now();
        let file = tokio::fs::File::open(&self.zip_path).await
            .map_err(|e| AnalysisError::Io { source: e, offset: 0 })?;
        
        let file_size = file.metadata().await
            .map_err(|e| AnalysisError::Io { source: e, offset: 0 })?.len();
        
        let (progress_tx, _) = mpsc::channel(100);
        let (chunk_results_tx, mut chunk_results_rx) = mpsc::channel(100);
        let (control_tx, mut control_rx) = watch::channel(ControlSignal::Continue);

        let mut chunk_results = Vec::new();
        let mut current_offset = 0;
        let mut buffer = vec![0; self.chunk_size];

        let mut file = tokio::io::BufReader::new(file);

        while current_offset < file_size {
            // Check for control signals
            if let Ok(signal) = control_rx.has_changed() {
                match control_rx.borrow().clone() {
                    ControlSignal::Stop { graceful: true } => break,
                    ControlSignal::Stop { graceful: false } => {
                        return Err(AnalysisError::Progress { 
                            msg: "Analysis interrupted".to_string() 
                        });
                    },
                    ControlSignal::Pause => {
                        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                        continue;
                    },
                    ControlSignal::Continue => {}
                }
            }

            let bytes_read = file.read(&mut buffer).await
                .map_err(|e| AnalysisError::Io { 
                    source: e, 
                    offset: current_offset 
                })?;

            if bytes_read == 0 {
                break;
            }

            let chunk = &buffer[..bytes_read];
            let result = self.chunk_processor.process_chunk(chunk, current_offset).await?;
            
            // Update progress
            progress_tx.send(ProgressUpdate {
                bytes_processed: current_offset + bytes_read as u64,
                files_processed: result.files.len(),
                current_file: result.files.last()
                    .map(|f| f.path.display().to_string())
                    .unwrap_or_default(),
                compression_ratio: result.compressed_size as f64 / result.uncompressed_size as f64,
                estimated_remaining_secs: ((file_size - current_offset) * start_time.elapsed().as_secs() as u64) / current_offset,
                error_count: if result.error.is_some() { 1 } else { 0 },
            }).await.map_err(|e| AnalysisError::Progress { 
                msg: e.to_string() 
            })?;

            chunk_results.push(result);
            current_offset += bytes_read as u64;
        }

        let duration = start_time.elapsed();
        let mut analysis = ChunkProcessor::merge_results(&chunk_results)?;
        analysis.stats.duration = duration;
        
        Ok(analysis)
    }
}


=== ./src/analyzer/mod.rs ===

mod zip;
mod chunks;

pub use zip::ParallelZipAnalyzer;
pub use chunks::{ChunkConfig, ChunkProcessor, ChunkResult};


=== ./src/writer/progress.rs ===

use std::sync::Arc;
use indicatif::{ProgressBar, ProgressStyle};
use tokio::sync::RwLock;
use crate::error::Result;

#[derive(Debug, Clone)]
pub struct ProgressUpdate {
    pub bytes_processed: u64,
    pub files_processed: usize,
    pub current_file: String,
    pub compression_ratio: f64,
    pub estimated_remaining_secs: u64,
    pub error_count: usize,
}

pub struct ProgressConfig {
    pub update_frequency_ms: u64,
    pub style_template: String,
    pub refresh_rate: std::time::Duration,
}

impl Default for ProgressConfig {
    fn default() -> Self {
        Self {
            update_frequency_ms: 100,
            style_template: String::from("[{elapsed_precise}] {bar:40.cyan/blue} {pos:>7}/{len:7} {msg}"),
            refresh_rate: std::time::Duration::from_millis(33),
        }
    }
}

#[derive(Debug, Default, Clone)]
pub struct ProgressStats {
    pub total_bytes: u64,
    pub processed_bytes: u64,
    pub total_files: usize,
    pub processed_files: usize,
    pub error_count: usize,
}

#[derive(Clone)]
pub struct ProgressTracker {
    bar: Arc<ProgressBar>,
    stats: Arc<RwLock<ProgressStats>>,
    config: ProgressConfig,
}

impl ProgressTracker {
    pub fn new(total_size: u64, config: ProgressConfig) -> Self {
        let bar = ProgressBar::new(total_size);
        bar.set_style(
            ProgressStyle::default_bar()
                .template(&config.style_template)
                .unwrap()
                .progress_chars("=>-")
        );
        bar.enable_steady_tick(config.refresh_rate);

        Self {
            bar,
            stats: Arc::new(RwLock::new(ProgressStats::default())),
            config,
        }
    }

    pub async fn update(&self, update: ProgressUpdate) -> Result<()> {
        let mut stats = self.stats.write().await;
        stats.processed_bytes = update.bytes_processed;
        stats.processed_files = update.files_processed;
        stats.error_count = update.error_count;

        self.bar.set_position(update.bytes_processed);
        self.bar.set_message(format!(
            "Processing: {} ({:.1}% compressed)", 
            update.current_file,
            update.compression_ratio * 100.0
        ));

        Ok(())
    }

    pub fn finish(self) -> Result<()> {
        self.bar.finish_with_message("Analysis complete!");
        Ok(())
    }
}


=== ./src/writer/report.rs ===

use std::path::PathBuf;
use tokio::fs::File;
use tokio::io::AsyncWriteExt;
use humansize::{format_size, BINARY};
use chrono::SecondsFormat;
use crate::{
    error::Result,
    models::{ZipAnalysis, FileInfo},
};

#[derive(Debug, Clone, Copy)]
pub enum SizeFormat {
    Binary,
    Decimal,
    Bytes,
}

#[derive(Debug, Clone, Copy)]
pub enum SortField {
    Path,
    Size,
    CompressedSize,
    CompressionRatio,
    Modified,
}

pub struct FormatConfig {
    pub include_headers: bool,
    pub size_format: SizeFormat,
    pub sort_by: SortField,
    pub timestamp_format: String,
}

impl Default for FormatConfig {
    fn default() -> Self {
        Self {
            include_headers: true,
            size_format: SizeFormat::Binary,
            sort_by: SortField::Path,
            timestamp_format: String::from("%Y-%m-%d %H:%M:%S"),
        }
    }
}

pub struct ReportWriter {
    output_path: PathBuf,
    format_config: FormatConfig,
}

impl ReportWriter {
    pub fn new(path: PathBuf, config: FormatConfig) -> Self {
        Self {
            output_path: path,
            format_config: config,
        }
    }

    pub async fn write(&self, analysis: &ZipAnalysis) -> Result<()> {
        let mut file = File::create(&self.output_path).await?;
        
        // Write header
        if self.format_config.include_headers {
            let header = self.format_header(analysis);
            file.write_all(header.as_bytes()).await?;
        }

        // Sort and write files
        let mut files = analysis.files.clone();
        self.sort_files(&mut files);

        for file_info in files {
            let line = self.format_file_entry(&file_info);
            file.write_all(line.as_bytes()).await?;
        }

        // Write summary
        let summary = self.format_summary(analysis);
        file.write_all(summary.as_bytes()).await?;

        Ok(())
    }

    fn format_header(&self, analysis: &ZipAnalysis) -> String {
        format!(
            "ZIP File Analysis Report\n\
             ====================\n\
             Analysis Duration: {}ms\n\
             Chunks Processed: {}\n\
             Peak Memory Usage: {}MB\n\
             Error Count: {}\n\n",
            analysis.stats.duration.as_millis(),
            analysis.stats.chunks_processed.load(Ordering::Relaxed),
            analysis.stats.peak_memory_mb.load(Ordering::Relaxed),
            analysis.stats.error_count.load(Ordering::Relaxed),
        )
    }

    fn format_file_entry(&self, file_info: &FileInfo) -> String {
        let size = match self.format_config.size_format {
            SizeFormat::Binary => format_size(file_info.size, BINARY),
            SizeFormat::Decimal => format_size(file_info.size, humansize::DECIMAL),
            SizeFormat::Bytes => file_info.size.to_string(),
        };

        let modified = file_info.modified.to_rfc3339_opts(SecondsFormat::Secs, true);

        format!(
            "{}\t{}\t{}\t{:?}\t{}\n",
            file_info.path.display(),
            size,
            file_info.compression_method,
            file_info.crc32,
            modified,
        )
    }

    fn format_summary(&self, analysis: &ZipAnalysis) -> String {
        format!(
            "\nSummary\n\
             =======\n\
             Total Size: {}\n\
             Compressed Size: {}\n\
             Compression Ratio: {:.2}%\n\
             Total Files: {}\n",
            format_size(analysis.total_size, BINARY),
            format_size(analysis.compressed_size, BINARY),
            (1.0 - analysis.compression_ratio) * 100.0,
            analysis.files.len(),
        )
    }

    fn sort_files(&self, files: &mut Vec<FileInfo>) {
        match self.format_config.sort_by {
            SortField::Path => files.sort_by(|a, b| a.path.cmp(&b.path)),
            SortField::Size => files.sort_by(|a, b| b.size.cmp(&a.size)),
            SortField::CompressedSize => files.sort_by(|a, b| b.compressed_size.cmp(&a.compressed_size)),
            SortField::CompressionRatio => files.sort_by(|a, b| {
                let ratio_a = a.compressed_size as f64 / a.size as f64;
                let ratio_b = b.compressed_size as f64 / b.size as f64;
                ratio_b.partial_cmp(&ratio_a).unwrap_or(std::cmp::Ordering::Equal)
            }),
            SortField::Modified => files.sort_by(|a, b| b.modified.cmp(&a.modified)),
        }
    }
}


=== ./src/writer/mod.rs ===

mod progress;
mod report;

pub use progress::{ProgressTracker, ProgressConfig, ProgressUpdate};
pub use report::{ReportWriter, FormatConfig, SizeFormat, SortField};
